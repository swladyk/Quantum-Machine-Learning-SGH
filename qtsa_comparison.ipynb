{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a91e3e95",
   "metadata": {},
   "source": [
    "Stanis≈Çaw W≈Çadyka 114114\n",
    "\n",
    "https://github.com/swladyk/Quantum-Machine-Learning-SGH.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b31b2cc",
   "metadata": {},
   "source": [
    "\n",
    "## Wyniki Eksperymentu\n",
    "\n",
    "### Konfiguracja\n",
    "- **Ticker:** `NVDA`\n",
    "- **Okres:** 2020-01-01 do 2025-01-01\n",
    "- **Okno czasowe:** 20 dni\n",
    "- **Epoki:** 100\n",
    "- **Batch size:** 32\n",
    "- **Learning rate:** 0.01\n",
    "- **Seed:** 42\n",
    "\n",
    "### Finalne wyniki na zbiorze testowym\n",
    "\n",
    "| Model | Accuracy | Precision (UP) | Recall (UP) | Parametry (wagi) |\n",
    "|-------|----------|----------------|-------------|-----------|\n",
    "| **QTSA (Quantum)** | **57.7%** | 62.2% | 60.9% | 63 |\n",
    "| **MLP (Classical)** | **57.7%** | 59.4% | 75.4% | ~1,100 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     formats: ipynb,py:percent\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: percent\n",
    "#       format_version: '1.3'\n",
    "#       jupytext_version: 1.18.1\n",
    "#   kernelspec:\n",
    "#     display_name: .venv (3.13.11)\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97d023a",
   "metadata": {},
   "source": [
    "# QTSA: Binary Classification - Quantum vs Classical\n",
    "\n",
    "## Professional Financial ML Approach for SGH Final Project\n",
    "\n",
    "**Problem:** Predicting market direction (Up/Down) as a **Binary Classification Task**.\n",
    "\n",
    "### Why Classification, not Regression?\n",
    "\n",
    "In professional Financial ML, we **DO NOT** predict raw prices/returns because:\n",
    "1. **Non-stationarity:** Asset prices are non-stationary random walks\n",
    "2. **Noise fitting:** Regression on prices leads to overfitting to market noise\n",
    "3. **Business value:** Traders care about **direction** (buy/sell signal), not exact price\n",
    "\n",
    "### Our Approach:\n",
    "- **Feature:** Stationary log-returns, normalized to [0, œÄ] for quantum gates\n",
    "- **Target:** Binary label: `1` if next return > 0 (market goes UP), else `0` (DOWN)\n",
    "- **Loss:** Binary Cross-Entropy (BCE)\n",
    "- **Metrics:** Accuracy, Confusion Matrix\n",
    "- **Models:** QTSA (1-qubit Serial Re-uploading) vs Classical MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20acf013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba9f39e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381b830",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "TICKER = \"NVDA\"  # Nvidia - strong trends, good for classification\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE = \"2025-01-01\"\n",
    "WINDOW_SIZE = 20  # 20 days of historical log-returns\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Ticker: {TICKER}\")\n",
    "print(f\"  Date range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"  Window size: {WINDOW_SIZE} days\")\n",
    "print(f\"  Task: Binary Classification (Market Direction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b5417",
   "metadata": {},
   "source": [
    "## Quantum Architecture: Serial Data Re-uploading for Classification\n",
    "\n",
    "### Adapted QTSA for Binary Classification\n",
    "\n",
    "The quantum circuit remains the same (serial re-uploading), but the **output interpretation** changes:\n",
    "\n",
    "```\n",
    "|0‚ü© ‚îÄ‚îÄ‚îÄ Rot(Œ∏‚ÇÄ) ‚îÄ‚îÄ‚îÄ RX(x‚ÇÄ) ‚îÄ‚îÄ‚îÄ Rot(Œ∏‚ÇÅ) ‚îÄ‚îÄ‚îÄ RX(x‚ÇÅ) ‚îÄ‚îÄ‚îÄ ... ‚îÄ‚îÄ‚îÄ Rot(Œ∏_L) ‚îÄ‚îÄ‚îÄ ‚ü®Z‚ü©\n",
    "```\n",
    "\n",
    "**Key change:** Map ‚ü®Z‚ü© ‚àà [-1, 1] to probability P(UP) ‚àà [0, 1]:\n",
    "\n",
    "```\n",
    "P(market goes UP) = (‚ü®Z‚ü© + 1) / 2\n",
    "```\n",
    "\n",
    "- ‚ü®Z‚ü© = +1 ‚Üí P(UP) = 1.0 (100% confident: market goes UP)\n",
    "- ‚ü®Z‚ü© = -1 ‚Üí P(UP) = 0.0 (100% confident: market goes DOWN)\n",
    "- ‚ü®Z‚ü© = 0  ‚Üí P(UP) = 0.5 (uncertain)\n",
    "\n",
    "### Comparison: Classification Task\n",
    "\n",
    "| Aspect | QTSA (Quantum) | MLP (Classical) |\n",
    "|--------|----------------|-----------------|\n",
    "| **Task** | Binary Classification | Binary Classification |\n",
    "| **Architecture** | 1 qubit, serial re-uploading | 3-layer FC with Sigmoid |\n",
    "| **Parameters** | 63 (L=20) | ~1,100 |\n",
    "| **Output** | Probability from ‚ü®Z‚ü© | Probability from Sigmoid |\n",
    "| **Loss** | BCE Loss | BCE Loss |\n",
    "| **Metric** | Accuracy, Confusion Matrix | Accuracy, Confusion Matrix |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f51623",
   "metadata": {},
   "source": [
    "### Visualization: Quantum Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1059d7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def draw_quantum_circuit(window_size_demo: int = 5):\n",
    "    \"\"\"Visualize QTSA circuit for demonstration.\"\"\"\n",
    "    # Use local generator to avoid affecting global random state\n",
    "    generator = torch.Generator().manual_seed(999)\n",
    "    \n",
    "    dev = qml.device(\"default.qubit\", wires=1)\n",
    "    \n",
    "    @qml.qnode(dev)\n",
    "    def demo_circuit(inputs, weights):\n",
    "        for i in range(window_size_demo):\n",
    "            qml.Rot(weights[i, 0], weights[i, 1], weights[i, 2], wires=0)\n",
    "            qml.RX(inputs[i], wires=0)\n",
    "        qml.Rot(weights[-1, 0], weights[-1, 1], weights[-1, 2], wires=0)\n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "    \n",
    "    dummy_inputs = torch.tensor([0.5, 1.0, 1.5, 2.0, 2.5], dtype=torch.float32)\n",
    "    dummy_weights = torch.randn(window_size_demo + 1, 3, dtype=torch.float32, generator=generator)\n",
    "    \n",
    "    _ = demo_circuit(dummy_inputs, dummy_weights)\n",
    "    \n",
    "    fig, ax = qml.draw_mpl(demo_circuit, expansion_strategy=\"device\")(dummy_inputs, dummy_weights)\n",
    "    plt.tight_layout()\n",
    "    plt.title(f\"QTSA Circuit for Classification (L={window_size_demo})\")\n",
    "    return fig\n",
    "\n",
    "try:\n",
    "    fig_circuit = draw_quantum_circuit(window_size_demo=5)\n",
    "    plt.show()\n",
    "    print(f\"‚úì Circuit visualization for {5} timesteps (simplified)\")\n",
    "    print(f\"‚úì Actual model uses WINDOW_SIZE={WINDOW_SIZE}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Cannot draw circuit: {e}\")\n",
    "    print(\"  (Normal if no graphical backend available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c16bb8",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b9ee4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def download_close_prices(ticker: str, start: str, end: str) -> pd.Series:\n",
    "    \"\"\"Download closing prices from Yahoo Finance.\"\"\"\n",
    "    print(f\"Downloading {ticker} data...\")\n",
    "    df = yf.download(ticker, start=start, end=end, progress=False)\n",
    "    if df.empty or \"Close\" not in df:\n",
    "        raise ValueError(f\"No Close data for ticker {ticker}.\")\n",
    "    prices = df[\"Close\"].dropna()\n",
    "    \n",
    "    # Handle multi-level columns from yfinance (convert to Series if needed)\n",
    "    if isinstance(prices, pd.DataFrame):\n",
    "        prices = prices.squeeze()\n",
    "    \n",
    "    print(f\"  ‚úì Downloaded {len(prices)} data points\")\n",
    "    return prices\n",
    "\n",
    "\n",
    "prices = download_close_prices(TICKER, START_DATE, END_DATE)\n",
    "print(f\"\\nPrice statistics:\")\n",
    "print(prices.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29d2434",
   "metadata": {},
   "source": [
    "## Feature Engineering: Professional Financial ML\n",
    "\n",
    "### Step 1: Compute Log-Returns (Stationary Feature)\n",
    "\n",
    "Log-returns are stationary and commonly used in quantitative finance:\n",
    "\n",
    "```\n",
    "r_t = log(P_t / P_{t-1})\n",
    "```\n",
    "\n",
    "### Step 2: Normalize to [0, œÄ] for Quantum Encoding\n",
    "\n",
    "We use **RobustScaler** (not MinMaxScaler) to handle outliers:\n",
    "- RobustScaler uses median and IQR, robust to extreme market events\n",
    "- Then clip to [0, œÄ] range required by RX gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c201b1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_log_returns(prices: pd.Series) -> pd.Series:\n",
    "    \"\"\"Compute log-returns: r_t = log(P_t / P_{t-1})\"\"\"\n",
    "    returns = np.log(prices / prices.shift(1)).dropna()\n",
    "    print(f\"Log-returns computed: {len(returns)} points\")\n",
    "    print(f\"  Mean: {float(returns.mean()):.6f}\")\n",
    "    print(f\"  Std: {float(returns.std()):.6f}\")\n",
    "    print(f\"  Min: {float(returns.min()):.6f}, Max: {float(returns.max()):.6f}\")\n",
    "    return returns\n",
    "\n",
    "\n",
    "def normalize_to_pi_range(returns: pd.Series) -> Tuple[np.ndarray, RobustScaler]:\n",
    "    \"\"\"\n",
    "    Normalize returns to [0, œÄ] using RobustScaler.\n",
    "    RobustScaler is better than MinMaxScaler for financial data (outlier-robust).\n",
    "    \"\"\"\n",
    "    scaler = RobustScaler()\n",
    "    values = returns.values.reshape(-1, 1)\n",
    "    \n",
    "    # Scale using median and IQR\n",
    "    scaled = scaler.fit_transform(values).flatten()\n",
    "    \n",
    "    # Map to [0, œÄ] range: shift to positive, then scale\n",
    "    # Use percentile clipping to handle extreme outliers\n",
    "    p_low, p_high = np.percentile(scaled, [1, 99])\n",
    "    scaled_clipped = np.clip(scaled, p_low, p_high)\n",
    "    \n",
    "    # Normalize to [0, œÄ]\n",
    "    min_val = scaled_clipped.min()\n",
    "    max_val = scaled_clipped.max()\n",
    "    normalized = (scaled_clipped - min_val) / (max_val - min_val + 1e-9) * np.pi\n",
    "    \n",
    "    print(f\"Normalization to [0, œÄ]:\")\n",
    "    print(f\"  Range: [{float(normalized.min()):.4f}, {float(normalized.max()):.4f}]\")\n",
    "    print(f\"  Mean: {float(normalized.mean()):.4f}, Std: {float(normalized.std()):.4f}\")\n",
    "    \n",
    "    return normalized.astype(np.float32), scaler\n",
    "\n",
    "\n",
    "log_returns = compute_log_returns(prices)\n",
    "normalized_returns, scaler = normalize_to_pi_range(log_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34889019",
   "metadata": {},
   "source": [
    "## Target Engineering: Binary Labels\n",
    "\n",
    "**Target:** Predict if the NEXT return will be positive (UP) or negative (DOWN)\n",
    "\n",
    "```\n",
    "y_t = 1  if  r_{t+1} > 0  (market goes UP)\n",
    "y_t = 0  if  r_{t+1} ‚â§ 0  (market goes DOWN)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f516f4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_binary_target(returns: pd.Series) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create binary classification target:\n",
    "    1 if next return > 0 (UP), 0 otherwise (DOWN)\n",
    "    \"\"\"\n",
    "    # Shift returns backward by 1 to get \"next day's return\"\n",
    "    next_returns = returns.shift(-1).dropna()\n",
    "    binary_target = (next_returns > 0).astype(np.float32).values\n",
    "    \n",
    "    print(f\"Binary target created:\")\n",
    "    print(f\"  Total samples: {len(binary_target)}\")\n",
    "    print(f\"  UP (1): {float(binary_target.sum()):.0f} ({float(binary_target.mean()):.1%})\")\n",
    "    print(f\"  DOWN (0): {float((1-binary_target).sum()):.0f} ({float((1-binary_target.mean())):.1%})\")\n",
    "    \n",
    "    return binary_target\n",
    "\n",
    "\n",
    "# Need to align: normalized_returns and binary_target must match in length\n",
    "# binary_target is for predicting the NEXT return\n",
    "binary_target = create_binary_target(log_returns)\n",
    "\n",
    "# Align lengths: remove last element from normalized_returns (no future target for it)\n",
    "normalized_returns = normalized_returns[:len(binary_target)]\n",
    "\n",
    "print(f\"\\nFinal aligned data:\")\n",
    "print(f\"  Features (normalized returns): {len(normalized_returns)}\")\n",
    "print(f\"  Targets (binary labels): {len(binary_target)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4547fe2",
   "metadata": {},
   "source": [
    "## Sequence Building: Sliding Window\n",
    "\n",
    "Create sequences of length `WINDOW_SIZE`:\n",
    "- **X:** [r_{t-L}, r_{t-L+1}, ..., r_{t-1}] (normalized)\n",
    "- **y:** Binary label for r_t (1=UP, 0=DOWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773d1c8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_classification_sequences(\n",
    "    features: np.ndarray, targets: np.ndarray, window_size: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build sliding window sequences for classification.\n",
    "    \n",
    "    Args:\n",
    "        features: Normalized log-returns [N]\n",
    "        targets: Binary labels [N]\n",
    "        window_size: Length of input sequence\n",
    "        \n",
    "    Returns:\n",
    "        X: [N-window_size, window_size]\n",
    "        y: [N-window_size] binary labels\n",
    "    \"\"\"\n",
    "    if len(features) != len(targets):\n",
    "        raise ValueError(\"Features and targets must have same length\")\n",
    "    \n",
    "    if len(features) <= window_size:\n",
    "        raise ValueError(\"Not enough data for given window_size\")\n",
    "    \n",
    "    X_seq, y_seq = [], []\n",
    "    for idx in range(window_size, len(features)):\n",
    "        X_seq.append(features[idx - window_size : idx])\n",
    "        y_seq.append(targets[idx])\n",
    "    \n",
    "    X_seq = np.stack(X_seq)\n",
    "    y_seq = np.array(y_seq, dtype=np.float32)\n",
    "    \n",
    "    print(f\"Sequences built:\")\n",
    "    print(f\"  X shape: {X_seq.shape}\")\n",
    "    print(f\"  y shape: {y_seq.shape}\")\n",
    "    print(f\"  Class balance in y: UP={float(y_seq.mean()):.1%}, DOWN={float(1-y_seq.mean()):.1%}\")\n",
    "    \n",
    "    return X_seq, y_seq\n",
    "\n",
    "\n",
    "X_all, y_all = build_classification_sequences(normalized_returns, binary_target, WINDOW_SIZE)\n",
    "\n",
    "# Split chronologically (80/20)\n",
    "split_idx = int(len(X_all) * 0.8)\n",
    "X_train, y_train = X_all[:split_idx], y_all[:split_idx]\n",
    "X_test, y_test = X_all[split_idx:], y_all[split_idx:]\n",
    "\n",
    "print(f\"\\nTrain/Test split:\")\n",
    "print(f\"  Train: {len(X_train)} samples, UP={float(y_train.mean()):.1%}\")\n",
    "print(f\"  Test:  {len(X_test)} samples, UP={float(y_test.mean()):.1%}\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_train), torch.tensor(y_train)),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Keep chronological order for time series\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_test), torch.tensor(y_test)),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be05d3cf",
   "metadata": {},
   "source": [
    "## Quantum Model: QTSA for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714fbfd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class QTSAClassifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    QTSA model adapted for binary classification.\n",
    "    Output: Probability P(market goes UP) ‚àà [0, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int, device_name: str = \"default.qubit\"):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.dev = qml.device(device_name, wires=1)\n",
    "        weight_shapes = {\"weights\": (window_size + 1, 3)}\n",
    "        self.qnode = self._build_qnode()\n",
    "        self.qlayer = qml.qnn.TorchLayer(self.qnode, weight_shapes)\n",
    "\n",
    "    def _build_qnode(self):\n",
    "        window_size = self.window_size\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"torch\", diff_method=\"backprop\")\n",
    "        def qtsa_circuit(inputs, weights):\n",
    "            # Serial data re-uploading: Rot ‚Üí RX for each timestep\n",
    "            for i in range(window_size):\n",
    "                qml.Rot(weights[i, 0], weights[i, 1], weights[i, 2], wires=0)\n",
    "                qml.RX(inputs[..., i], wires=0)\n",
    "            # Final variational layer\n",
    "            qml.Rot(weights[-1, 0], weights[-1, 1], weights[-1, 2], wires=0)\n",
    "            return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "        return qtsa_circuit\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: return probability P(UP).\n",
    "        \n",
    "        ‚ü®Z‚ü© ‚àà [-1, 1] ‚Üí P(UP) = (‚ü®Z‚ü© + 1) / 2 ‚àà [0, 1]\n",
    "        \"\"\"\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        x = x.to(dtype=torch.float32)\n",
    "        z = self.qlayer(x)  # ‚ü®Z‚ü© ‚àà [-1, 1]\n",
    "        prob_up = (z + 1.0) * 0.5  # Map to [0, 1]\n",
    "        return prob_up.squeeze()\n",
    "\n",
    "\n",
    "print(\"QTSA Classifier Architecture:\")\n",
    "print(f\"  Input: {WINDOW_SIZE} normalized log-returns ‚àà [0, œÄ]\")\n",
    "print(f\"  Circuit: Serial re-uploading with {WINDOW_SIZE+1} Rot layers\")\n",
    "print(f\"  Output: P(market UP) ‚àà [0, 1]\")\n",
    "print(f\"  Parameters: {(WINDOW_SIZE+1) * 3} trainable rotation angles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83007b4",
   "metadata": {},
   "source": [
    "## Classical Model: MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b056f2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Classical MLP baseline for binary classification.\n",
    "    Output: Probability P(market goes UP) via Sigmoid.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(window_size, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(32, 1),\n",
    "            torch.nn.Sigmoid(),  # Output probability\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        x = x.to(dtype=torch.float32)\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "# Count parameters\n",
    "mlp_params = sum(p.numel() for p in MLPClassifier(WINDOW_SIZE).parameters())\n",
    "qtsa_params = (WINDOW_SIZE + 1) * 3\n",
    "\n",
    "print(\"MLP Classifier Architecture:\")\n",
    "print(f\"  Input: {WINDOW_SIZE} features\")\n",
    "print(f\"  Hidden: 64 ‚Üí 32 neurons with ReLU + Dropout\")\n",
    "print(f\"  Output: P(market UP) via Sigmoid\")\n",
    "print(f\"  Parameters: {mlp_params}\")\n",
    "print(f\"\\nParameter comparison: QTSA={qtsa_params} vs MLP={mlp_params} ({mlp_params/qtsa_params:.1f}x more)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afe523",
   "metadata": {},
   "source": [
    "## Training Function with Accuracy Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db129d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    model_name: str,\n",
    ") -> Tuple[List[float], List[float], List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train binary classification model.\n",
    "    \n",
    "    Returns:\n",
    "        train_loss_history, test_loss_history, train_acc_history, test_acc_history\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "    \n",
    "    train_loss_hist = []\n",
    "    test_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    test_acc_hist = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"[{model_name}] Epoch {epoch}/{epochs}\", leave=False)\n",
    "        for xb, yb in pbar:\n",
    "            xb = xb.to(dtype=torch.float32)\n",
    "            yb = yb.to(dtype=torch.float32)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            probs = model(xb)  # Probabilities\n",
    "            loss = loss_fn(probs, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * len(xb)\n",
    "            preds = (probs > 0.5).float()  # Binary predictions\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += len(xb)\n",
    "            \n",
    "            pbar.set_postfix(loss=loss.item(), acc=correct/total)\n",
    "\n",
    "        train_loss = total_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(dtype=torch.float32)\n",
    "                yb = yb.to(dtype=torch.float32)\n",
    "                probs = model(xb)\n",
    "                loss = loss_fn(probs, yb)\n",
    "                total_loss += loss.item() * len(xb)\n",
    "                preds = (probs > 0.5).float()\n",
    "                correct += (preds == yb).sum().item()\n",
    "                total += len(xb)\n",
    "        \n",
    "        test_loss = total_loss / total\n",
    "        test_acc = correct / total\n",
    "\n",
    "        train_loss_hist.append(train_loss)\n",
    "        test_loss_hist.append(test_loss)\n",
    "        train_acc_hist.append(train_acc)\n",
    "        test_acc_hist.append(test_acc)\n",
    "        \n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"[{model_name}] Epoch {epoch}/{epochs} | \"\n",
    "                f\"Train Loss={train_loss:.4f} Acc={train_acc:.3f} | \"\n",
    "                f\"Test Loss={test_loss:.4f} Acc={test_acc:.3f}\"\n",
    "            )\n",
    "\n",
    "    return train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7625933d",
   "metadata": {},
   "source": [
    "## Training: QTSA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training QTSA (Quantum) Classifier\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "qtsa_model = QTSAClassifier(window_size=WINDOW_SIZE)\n",
    "q_train_loss, q_test_loss, q_train_acc, q_test_acc = train_classifier(\n",
    "    qtsa_model, train_loader, test_loader, epochs=EPOCHS, lr=LR, model_name=\"QTSA\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì QTSA Training Complete\")\n",
    "print(f\"  Final Train Accuracy: {q_train_acc[-1]:.3f}\")\n",
    "print(f\"  Final Test Accuracy:  {q_test_acc[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c747d2e",
   "metadata": {},
   "source": [
    "## Training: Classical MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edadd31",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Classical MLP Classifier\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mlp_model = MLPClassifier(window_size=WINDOW_SIZE)\n",
    "cl_train_loss, cl_test_loss, cl_train_acc, cl_test_acc = train_classifier(\n",
    "    mlp_model, train_loader, test_loader, epochs=EPOCHS, lr=LR, model_name=\"MLP\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì MLP Training Complete\")\n",
    "print(f\"  Final Train Accuracy: {cl_train_acc[-1]:.3f}\")\n",
    "print(f\"  Final Test Accuracy:  {cl_test_acc[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab8129",
   "metadata": {},
   "source": [
    "## Predictions and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac398811",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_predictions(model: torch.nn.Module, loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Get predictions (probabilities and binary labels) from model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(dtype=torch.float32)\n",
    "            probs = model(xb)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_labels.append(yb.cpu().numpy())\n",
    "    \n",
    "    probs = np.concatenate(all_probs)\n",
    "    labels = np.concatenate(all_labels)\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    \n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "qtsa_preds, qtsa_labels = get_predictions(qtsa_model, test_loader)\n",
    "mlp_preds, mlp_labels = get_predictions(mlp_model, test_loader)\n",
    "\n",
    "qtsa_acc = accuracy_score(qtsa_labels, qtsa_preds)\n",
    "mlp_acc = accuracy_score(mlp_labels, mlp_preds)\n",
    "\n",
    "print(f\"\\nFinal Test Set Accuracy:\")\n",
    "print(f\"  QTSA (Quantum): {qtsa_acc:.3f} ({qtsa_acc:.1%})\")\n",
    "print(f\"  MLP (Classical): {mlp_acc:.3f} ({mlp_acc:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a778760",
   "metadata": {},
   "source": [
    "## Visualization: Results Comparison\n",
    "\n",
    "### 1. Confusion Matrices\n",
    "### 2. Accuracy Comparison\n",
    "### 3. Training History (Loss & Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaeabc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(\n",
    "    q_preds, q_labels, q_acc,\n",
    "    cl_preds, cl_labels, cl_acc,\n",
    "    q_train_loss, q_test_loss, q_train_acc, q_test_acc,\n",
    "    cl_train_loss, cl_test_loss, cl_train_acc, cl_test_acc,\n",
    "):\n",
    "    \"\"\"\n",
    "    Comprehensive visualization for classification results.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # === ROW 1: Confusion Matrices ===\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    cm_qtsa = confusion_matrix(q_labels, q_preds)\n",
    "    sns.heatmap(cm_qtsa, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "                xticklabels=['DOWN', 'UP'], yticklabels=['DOWN', 'UP'])\n",
    "    ax1.set_title(f'QTSA Confusion Matrix\\nAccuracy: {q_acc:.3f}', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('True Label')\n",
    "    ax1.set_xlabel('Predicted Label')\n",
    "    \n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    cm_mlp = confusion_matrix(cl_labels, cl_preds)\n",
    "    sns.heatmap(cm_mlp, annot=True, fmt='d', cmap='Oranges', ax=ax2,\n",
    "                xticklabels=['DOWN', 'UP'], yticklabels=['DOWN', 'UP'])\n",
    "    ax2.set_title(f'MLP Confusion Matrix\\nAccuracy: {cl_acc:.3f}', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('True Label')\n",
    "    ax2.set_xlabel('Predicted Label')\n",
    "    \n",
    "    # === ROW 1, COL 3: Accuracy Bar Chart ===\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    models = ['QTSA\\n(Quantum)', 'MLP\\n(Classical)']\n",
    "    accuracies = [q_acc, cl_acc]\n",
    "    colors = ['tab:blue', 'tab:orange']\n",
    "    bars = ax3.bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax3.set_ylim([0, 1])\n",
    "    ax3.set_ylabel('Test Accuracy', fontsize=11)\n",
    "    ax3.set_title('Model Comparison', fontsize=12, fontweight='bold')\n",
    "    ax3.axhline(0.5, color='red', linestyle='--', linewidth=1, label='Random Baseline')\n",
    "    ax3.legend()\n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # === ROW 2: Loss History ===\n",
    "    ax4 = fig.add_subplot(gs[1, :])\n",
    "    epochs_range = range(1, len(q_train_loss) + 1)\n",
    "    ax4.plot(epochs_range, q_train_loss, label='QTSA Train Loss', color='tab:blue', alpha=0.6)\n",
    "    ax4.plot(epochs_range, q_test_loss, label='QTSA Test Loss', color='tab:blue', linewidth=2)\n",
    "    ax4.plot(epochs_range, cl_train_loss, label='MLP Train Loss', color='tab:orange', alpha=0.6)\n",
    "    ax4.plot(epochs_range, cl_test_loss, label='MLP Test Loss', color='tab:orange', linewidth=2)\n",
    "    ax4.set_xlabel('Epoch', fontsize=11)\n",
    "    ax4.set_ylabel('Binary Cross-Entropy Loss', fontsize=11)\n",
    "    ax4.set_title('Training History: Loss', fontsize=12, fontweight='bold')\n",
    "    ax4.legend(loc='upper right')\n",
    "    ax4.grid(alpha=0.3)\n",
    "    \n",
    "    # === ROW 3: Accuracy History ===\n",
    "    ax5 = fig.add_subplot(gs[2, :])\n",
    "    ax5.plot(epochs_range, q_train_acc, label='QTSA Train Acc', color='tab:blue', alpha=0.6)\n",
    "    ax5.plot(epochs_range, q_test_acc, label='QTSA Test Acc', color='tab:blue', linewidth=2)\n",
    "    ax5.plot(epochs_range, cl_train_acc, label='MLP Train Acc', color='tab:orange', alpha=0.6)\n",
    "    ax5.plot(epochs_range, cl_test_acc, label='MLP Test Acc', color='tab:orange', linewidth=2)\n",
    "    ax5.axhline(0.5, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Random Baseline')\n",
    "    ax5.set_xlabel('Epoch', fontsize=11)\n",
    "    ax5.set_ylabel('Accuracy', fontsize=11)\n",
    "    ax5.set_title('Training History: Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylim([0.4, 1.0])\n",
    "    ax5.legend(loc='lower right')\n",
    "    ax5.grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(\n",
    "        f'QTSA vs Classical MLP: Binary Market Direction Classification\\n'\n",
    "        f'Ticker: {TICKER} | Window: {WINDOW_SIZE} days | Test Samples: {len(q_labels)}',\n",
    "        fontsize=14, fontweight='bold', y=0.995\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_results(\n",
    "    qtsa_preds, qtsa_labels, qtsa_acc,\n",
    "    mlp_preds, mlp_labels, mlp_acc,\n",
    "    q_train_loss, q_test_loss, q_train_acc, q_test_acc,\n",
    "    cl_train_loss, cl_test_loss, cl_train_acc, cl_test_acc,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8775dd",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Professional ML Approach:**\n",
    "   - We framed the problem as **binary classification** (not regression)\n",
    "   - Used **stationary features** (log-returns) to avoid non-stationarity issues\n",
    "   - Evaluated using **accuracy** and **confusion matrices** (not MSE)\n",
    "\n",
    "2. **Model Comparison:**\n",
    "   - **QTSA:** Quantum model with only ~63 parameters\n",
    "   - **MLP:** Classical baseline with ~1,100 parameters (17x more)\n",
    "   - Both models predict market direction (UP/DOWN) from historical returns\n",
    "\n",
    "3. **Performance:**\n",
    "   - Compare test accuracy against random baseline (50%)\n",
    "   - Check confusion matrices for bias (e.g., always predicting UP)\n",
    "   - QTSA's advantage: extreme parameter efficiency\n",
    "\n",
    "4. **Why This Approach is Better:**\n",
    "   - ‚úì Addresses **non-stationarity** (log-returns are stationary)\n",
    "   - ‚úì Focuses on **actionable signal** (direction, not exact price)\n",
    "   - ‚úì Proper **financial ML metrics** (accuracy, not MSE on prices)\n",
    "   - ‚úì Reduces **overfitting risk** (fewer parameters in QTSA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "204504ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE\n",
      "============================================================\n",
      "‚úì Task: Binary Classification (Market Direction)\n",
      "‚úì Ticker: NVDA\n",
      "‚úì Models: QTSA (Quantum) vs MLP (Classical)\n",
      "‚úì Final Test Accuracy:\n",
      "    QTSA: 0.577\n",
      "    MLP:  0.577\n",
      "‚úì Visualization: Confusion matrices, accuracy comparison, training history\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úì Task: Binary Classification (Market Direction)\")\n",
    "print(f\"‚úì Ticker: {TICKER}\")\n",
    "print(f\"‚úì Models: QTSA (Quantum) vs MLP (Classical)\")\n",
    "print(f\"‚úì Final Test Accuracy:\")\n",
    "print(f\"    QTSA: {qtsa_acc:.3f}\")\n",
    "print(f\"    MLP:  {mlp_acc:.3f}\")\n",
    "print(f\"‚úì Visualization: Confusion matrices, accuracy comparison, training history\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad100aa",
   "metadata": {},
   "source": [
    "## üìä Markdown Results Summary\n",
    "\n",
    "**Poni≈ºej znajduje siƒô podsumowanie wynik√≥w w formacie Markdown.**  \n",
    "**Skopiuj i wklej na poczƒÖtek notebooka jako kom√≥rkƒô Markdown z wynikami eksperymentu.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "9abe8822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COPY THE TEXT BELOW AND PASTE AT THE TOP OF NOTEBOOK AS MARKDOWN CELL\n",
      "================================================================================\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "## üéØ Wyniki Eksperymentu\n",
      "\n",
      "### Konfiguracja\n",
      "- **Ticker:** `NVDA`\n",
      "- **Okres:** 2020-01-01 do 2025-01-01\n",
      "- **Okno czasowe:** 20 dni\n",
      "- **Epoki:** 100\n",
      "- **Batch size:** 32\n",
      "- **Learning rate:** 0.01\n",
      "- **Seed:** 42\n",
      "\n",
      "### üìà Finalne wyniki na zbiorze testowym\n",
      "\n",
      "| Model | Accuracy | Precision (UP) | Recall (UP) | Parametry |\n",
      "|-------|----------|----------------|-------------|-----------|\n",
      "| **QTSA (Quantum)** | **57.7%** | 62.2% | 60.9% | 63 |\n",
      "| **MLP (Classical)** | **57.7%** | 59.4% | 75.4% | ~1,100 |\n",
      "| Random Baseline | 50.0% | - | - | - |\n",
      "\n",
      "### üîç Confusion Matrix - QTSA\n",
      "\n",
      "|           | Pred DOWN | Pred UP |\n",
      "|-----------|-----------|---------|\n",
      "| **True DOWN** | 59 | 51 |\n",
      "| **True UP**   | 54 | 84 |\n",
      "\n",
      "### üîç Confusion Matrix - MLP\n",
      "\n",
      "|           | Pred DOWN | Pred UP |\n",
      "|-----------|-----------|---------|\n",
      "| **True DOWN** | 39 | 71 |\n",
      "| **True UP**   | 34 | 104 |\n",
      "\n",
      "### üí° Wnioski\n",
      "\n",
      "1. **Efektywno≈õƒá parametryczna:** QTSA osiƒÖga 57.7% accuracy z **17√ó mniejszƒÖ liczbƒÖ parametr√≥w** ni≈º MLP\n",
      "2. **Przewaga nad losowym baseline:** Oba modele przewy≈ºszajƒÖ losowe zgadywanie (50%)\n",
      "3. **Klasyfikacja binarna:** Zastosowanie professional Financial ML approach (log-returns, stationarity)\n",
      "4. **Architektura kwantowa:** Serial data re-uploading na 1 kubicie skutecznie koduje sekwencje czasowe\n",
      "\n",
      "---\n",
      "\n",
      "================================================================================\n",
      "END OF MARKDOWN SUMMARY\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COPY THE TEXT BELOW AND PASTE AT THE TOP OF NOTEBOOK AS MARKDOWN CELL\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Oblicz dodatkowe metryki z confusion matrix\n",
    "cm_qtsa = confusion_matrix(qtsa_labels, qtsa_preds)\n",
    "cm_mlp = confusion_matrix(mlp_labels, mlp_preds)\n",
    "\n",
    "qtsa_tn, qtsa_fp, qtsa_fn, qtsa_tp = cm_qtsa.ravel()\n",
    "mlp_tn, mlp_fp, mlp_fn, mlp_tp = cm_mlp.ravel()\n",
    "\n",
    "# Precision, Recall dla klasy UP (1)\n",
    "qtsa_precision = qtsa_tp / (qtsa_tp + qtsa_fp) if (qtsa_tp + qtsa_fp) > 0 else 0\n",
    "qtsa_recall = qtsa_tp / (qtsa_tp + qtsa_fn) if (qtsa_tp + qtsa_fn) > 0 else 0\n",
    "mlp_precision = mlp_tp / (mlp_tp + mlp_fp) if (mlp_tp + mlp_fp) > 0 else 0\n",
    "mlp_recall = mlp_tp / (mlp_tp + mlp_fn) if (mlp_tp + mlp_fn) > 0 else 0\n",
    "\n",
    "markdown_output = f\"\"\"\n",
    "---\n",
    "\n",
    "## üéØ Wyniki Eksperymentu\n",
    "\n",
    "### Konfiguracja\n",
    "- **Ticker:** `{TICKER}`\n",
    "- **Okres:** {START_DATE} do {END_DATE}\n",
    "- **Okno czasowe:** {WINDOW_SIZE} dni\n",
    "- **Epoki:** {EPOCHS}\n",
    "- **Batch size:** {BATCH_SIZE}\n",
    "- **Learning rate:** {LR}\n",
    "- **Seed:** {SEED}\n",
    "\n",
    "### üìà Finalne wyniki na zbiorze testowym\n",
    "\n",
    "| Model | Accuracy | Precision (UP) | Recall (UP) | Parametry |\n",
    "|-------|----------|----------------|-------------|-----------|\n",
    "| **QTSA (Quantum)** | **{qtsa_acc:.1%}** | {qtsa_precision:.1%} | {qtsa_recall:.1%} | 63 |\n",
    "| **MLP (Classical)** | **{mlp_acc:.1%}** | {mlp_precision:.1%} | {mlp_recall:.1%} | ~1,100 |\n",
    "| Random Baseline | 50.0% | - | - | - |\n",
    "\n",
    "### üîç Confusion Matrix - QTSA\n",
    "\n",
    "|           | Pred DOWN | Pred UP |\n",
    "|-----------|-----------|---------|\n",
    "| **True DOWN** | {qtsa_tn} | {qtsa_fp} |\n",
    "| **True UP**   | {qtsa_fn} | {qtsa_tp} |\n",
    "\n",
    "### üîç Confusion Matrix - MLP\n",
    "\n",
    "|           | Pred DOWN | Pred UP |\n",
    "|-----------|-----------|---------|\n",
    "| **True DOWN** | {mlp_tn} | {mlp_fp} |\n",
    "| **True UP**   | {mlp_fn} | {mlp_tp} |\n",
    "\n",
    "### üí° Wnioski\n",
    "\n",
    "1. **Efektywno≈õƒá parametryczna:** QTSA osiƒÖga {qtsa_acc:.1%} accuracy z **17√ó mniejszƒÖ liczbƒÖ parametr√≥w** ni≈º MLP\n",
    "2. **Przewaga nad losowym baseline:** Oba modele przewy≈ºszajƒÖ losowe zgadywanie (50%)\n",
    "3. **Klasyfikacja binarna:** Zastosowanie professional Financial ML approach (log-returns, stationarity)\n",
    "4. **Architektura kwantowa:** Serial data re-uploading na 1 kubicie skutecznie koduje sekwencje czasowe\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "print(markdown_output)\n",
    "print(\"=\" * 80)\n",
    "print(\"END OF MARKDOWN SUMMARY\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
